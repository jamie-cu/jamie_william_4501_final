{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading: programmatically download the Yellow Taxi & High-Volume For-Hire Vehicle (HVFHV) trip data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_URL: str = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "def get_taxi_html() -> str:\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html\n",
    "\n",
    "def find_taxi_parquet_links() -> List[str]:\n",
    "    ### BEGIN SOLUTION\n",
    "    html = get_taxi_html()\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    HVFHV_a_tags = soup.find_all(\"a\", attrs={\"title\": \"High Volume For-Hire Vehicle Trip Records\"})\n",
    "    yellow_a_tags = soup.find_all(\"a\", attrs={\"title\": \"Yellow Taxi Trip Records\"})\n",
    "    all_a_tags = HVFHV_a_tags + yellow_a_tags\n",
    "    return [a[\"href\"] for a in all_a_tags]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_urls(urls, start_year, start_month, end_year, end_month):\n",
    "    filtered_urls = []\n",
    "    for url in urls:\n",
    "        match = re.search(r'(\\d{4})-(\\d{2})', url)\n",
    "        if match:\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            if (start_year < year < end_year) or \\\n",
    "               (year == start_year and month >= start_month) or \\\n",
    "               (year == end_year and month <= end_month):\n",
    "                filtered_urls.append(url.strip())\n",
    "    return filtered_urls\n",
    "\n",
    "# Filtering URLs from January 2020 to August 2024\n",
    "urls = find_taxi_parquet_links()\n",
    "filtered_urls = filter_urls(urls, 2020, 1, 2024, 8)\n",
    "\n",
    "# Display the result\n",
    "urls = sorted(filtered_urls)\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = urls\n",
    "# Directory to save the downloaded files\n",
    "output_directory = \"Dataset\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to download a file\n",
    "def download_parquet_file(i, url, output_directory):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        \n",
    "        # Extract file name from the URL\n",
    "        file_name = url.split(\"/\")[-1]\n",
    "        file_path = os.path.join(output_directory, file_name)\n",
    "        \n",
    "        # Save the file\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"File {i} Downloaded: {file_name}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"File {i} Failed to download {url}. Error: {e}\")\n",
    "\n",
    "# Download all files\n",
    "for i, url in enumerate(urls):\n",
    "    download_parquet_file(i, url, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6405008, 6299367, 3007687, 238073, 348415, 549797, 800412, 1007286, 1341017, 1681132, 1509000, 1461898, 1369769, 1371709, 1925152, 2171187, 2507109, 2834264, 2821746, 2788757, 2963793, 3463504, 3472949, 3214369, 2463931, 2979431, 3627882, 3599920, 3588295, 3558124, 3174394, 3152677, 3183767, 3675411, 3252717, 3399549, 3066766, 2913955, 3403766, 3288250, 3513649, 3307234, 2907108, 2824209, 2846722, 3522285, 3339715, 3376567, 2964624, 3007526, 3582628, 3514289, 3723833, 3539193, 3076903, 2979183]\n",
      "Yellow Taxi Stable sample size: 664\n",
      "[20569368, 21725100, 13392928, 4312909, 6089999, 7555193, 9958454, 11096852, 12106669, 13268411, 11596865, 11637123, 11908468, 11613942, 14227393, 14111371, 14719171, 14961892, 15027174, 14499696, 14886055, 16545356, 16041639, 16054495, 14751591, 16019283, 18453548, 17752561, 18157335, 17780075, 17464619, 17185687, 17793551, 19306090, 18085896, 19665847, 18479031, 17960971, 20413539, 19144903, 19847676, 19366619, 19132131, 18322150, 19851123, 20186330, 19269250, 20516297, 19663930, 19359148, 21280788, 19733038, 20704538, 20123226, 19182934, 19128392]\n",
      "Uber Stable sample size: 664\n"
     ]
    }
   ],
   "source": [
    "def cochran_sample_size(N, p=0.5, e=0.05, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate Cochran's sample size for a finite population.\n",
    "    \n",
    "    Parameters:\n",
    "    N (int): Population size\n",
    "    p (float): Proportion of population (default=0.5 for max variability)\n",
    "    e (float): Margin of error (default=0.05 for ±5%)\n",
    "    confidence (float): Confidence level (default=0.95 for 95%)\n",
    "    \n",
    "    Returns:\n",
    "    int: Sample size\n",
    "    \"\"\"\n",
    "    # Z-score for the given confidence level\n",
    "    Z = {\n",
    "        0.90: 1.645,\n",
    "        0.95: 1.96,\n",
    "        0.99: 2.576\n",
    "    }.get(confidence, 1.96)  # Default to 95% confidence if not specified\n",
    "    \n",
    "    # Step 1: Cochran's formula for infinite population\n",
    "    n0 = (Z**2 * p * (1 - p)) / (e**2)\n",
    "    \n",
    "    # Step 2: Adjust for finite population\n",
    "    n = n0 / (1 + (n0 - 1) / N)\n",
    "    \n",
    "    return math.ceil(n)\n",
    "\n",
    "def stable_sample_size(monthly_populations, p=0.5, e=0.05, confidence=0.99, method='max'):\n",
    "    \"\"\"\n",
    "    Calculate a stable sample size across multiple months.\n",
    "    \n",
    "    Parameters:\n",
    "    monthly_populations (list): List of population sizes for each month\n",
    "    p (float): Proportion of population (default=0.5 for max variability)\n",
    "    e (float): Margin of error (default=0.05 for ±5%)\n",
    "    confidence (float): Confidence level (default=0.95 for 95%)\n",
    "    method (str): Aggregation method ('max', 'average', 'safety')\n",
    "    \n",
    "    Returns:\n",
    "    int: Stable sample size\n",
    "    \"\"\"\n",
    "    sample_sizes = [cochran_sample_size(N, p, e, confidence) for N in monthly_populations]\n",
    "    print(monthly_populations)\n",
    "    \n",
    "    if method == 'max':\n",
    "        return max(sample_sizes)\n",
    "    elif method == 'average':\n",
    "        return math.ceil(sum(sample_sizes) / len(sample_sizes))\n",
    "    elif method == 'safety':\n",
    "        return math.ceil(max(sample_sizes) * 1.1)  # Add a 10% safety margin\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'max', 'average', or 'safety'.\")\n",
    "\n",
    "## Yellow Taxi\n",
    "yellow_file_folder = 'Dataset/yellow_tripdata/'\n",
    "yellow_file_names = [f for f in os.listdir(yellow_file_folder) if os.path.isfile(os.path.join(yellow_file_folder, f))]\n",
    "\n",
    "monthly_populations_yellow = []\n",
    "for file in yellow_file_names:\n",
    "    parquet_file = pq.ParquetFile(yellow_file_folder + file)\n",
    "    monthly_populations_yellow.append(parquet_file.metadata.num_rows)\n",
    "    \n",
    "# Example monthly population sizes\n",
    "stable_size_yellow = stable_sample_size(monthly_populations_yellow, method='average')\n",
    "print(f\"Yellow Taxi Stable sample size: {stable_size_yellow}\")\n",
    "\n",
    "## Uber\n",
    "fhvhv_file_folder = 'Dataset/fhvhv_tripdata/'\n",
    "fhvhv_file_names = [f for f in os.listdir(fhvhv_file_folder) if os.path.isfile(os.path.join(fhvhv_file_folder, f))]\n",
    "\n",
    "monthly_populations_uber = []\n",
    "for file in fhvhv_file_names:\n",
    "    parquet_file = pq.ParquetFile(fhvhv_file_folder + file)\n",
    "    monthly_populations_uber.append(parquet_file.metadata.num_rows)\n",
    "    \n",
    "# Example monthly population sizes\n",
    "stable_size_uber = stable_sample_size(monthly_populations_uber, method='average')\n",
    "print(f\"Uber Stable sample size: {stable_size_uber}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = 'taxi_zones.shp'\n",
    "zones_gdf = gpd.read_file(shapefile_path)\n",
    "if zones_gdf.crs is None:\n",
    "    zones_gdf.set_crs(epsg=2263, inplace=True)  # Example: NY State Plane (EPSG:2263)\n",
    "zones_gdf = zones_gdf.to_crs(epsg=4326) # Reproject to WGS84 (Latitude/Longitude)\n",
    "\n",
    "# Calculate centroids for each zone (polygon)\n",
    "zones_gdf['centroid'] = zones_gdf.geometry.centroid\n",
    "zones_gdf['latitude'] = zones_gdf['centroid'].y\n",
    "zones_gdf['longitude'] = zones_gdf['centroid'].x\n",
    "\n",
    "# Retain only relevant columns: location ID, latitude, and longitude\n",
    "zones_df = zones_gdf[['LocationID', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow Taxi Filtering and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_yellow = \"Clean_Sampled_Dataset/Yellow/\"\n",
    "os.makedirs(output_directory_yellow, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(yellow_file_names[:]):\n",
    "    trips_df = pd.read_parquet(yellow_file_folder + file)\n",
    "    print('Current Processing:', i, file)\n",
    "\n",
    "    # Merge trip data with zone centroids for pickups and dropoffs\n",
    "    trips_with_pickup = trips_df.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='PULocationID',\n",
    "        right_on='LocationID'\n",
    "    ).rename(columns={'latitude': 'pickup_latitude', 'longitude': 'pickup_longitude'})\n",
    "\n",
    "    trips_with_locations = trips_with_pickup.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='DOLocationID',\n",
    "        right_on='LocationID',\n",
    "        suffixes=('', '_dropoff')\n",
    "    ).rename(columns={'latitude': 'dropoff_latitude', 'longitude': 'dropoff_longitude'})\n",
    "\n",
    "    # Filter out trips with invalid location IDs\n",
    "    valid_trips = trips_with_locations.dropna(subset=['pickup_latitude', 'dropoff_latitude'])\n",
    "\n",
    "    # Delete records that start_pos or end_pos is out of range\n",
    "    LAT_MIN, LAT_MAX = 40.560445, 40.908524\n",
    "    LON_MIN, LON_MAX = -74.242330, -73.717047\n",
    "    \n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['pickup_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['pickup_longitude'].between(LON_MIN, LON_MAX)) &\n",
    "        (valid_trips['dropoff_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['dropoff_longitude'].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "    ## Delete original locationID columns\n",
    "    valid_trips.drop(['PULocationID','DOLocationID','LocationID','LocationID_dropoff'],axis=1,inplace=True)\n",
    "    \n",
    "    valid_trips.columns = valid_trips.columns.str.lower()\n",
    "    \n",
    "    ## Delete records that trip_distance is missing or trip_distance <= 0, and convert datatype into Float\n",
    "    valid_trips = valid_trips.dropna(subset=['trip_distance'])\n",
    "    valid_trips = valid_trips[valid_trips['trip_distance']>0]\n",
    "    valid_trips['trip_distance'] = valid_trips['trip_distance'].astype(float)\n",
    "    \n",
    "    ## Delete records that passenger_count is missing or passenger_count <= 0, and convert datatype into Integer\n",
    "    valid_trips = valid_trips.dropna(subset=['passenger_count'])\n",
    "    valid_trips = valid_trips[valid_trips['passenger_count']>0]\n",
    "    valid_trips['passenger_count'] = valid_trips['passenger_count'].astype(int)\n",
    "    \n",
    "    ## Delete records that where Fare_amount, Total_amount, or Tolls_amount are negative.\n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['fare_amount']>=0) &\n",
    "        (valid_trips['total_amount']>=0) &\n",
    "        (valid_trips['tolls_amount']>=0)\n",
    "        ]\n",
    "    \n",
    "    ## Delete records that Payment_type not in the valid range (1-6), and convert datatype into Integer\n",
    "    valid_trips['payment_type'] = valid_trips['payment_type'].astype(int)\n",
    "    valid_trips = valid_trips[valid_trips['payment_type'].between(1,6)]\n",
    "    \n",
    "    ## Delete records that RateCodeID not in the valid range (1-6), and convert datatype into Integer\n",
    "    valid_trips['ratecodeid'] = valid_trips['ratecodeid'].astype(int)\n",
    "    valid_trips = valid_trips[valid_trips['ratecodeid'].between(1,6)]\n",
    "    valid_trips = valid_trips.rename(columns={'ratecodeid':'RateCodeID',})\n",
    "    \n",
    "    ## Convert store_and_fwd_flag into 0 and 1\n",
    "    valid_trips['store_and_fwd_flag'] = valid_trips['store_and_fwd_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    \n",
    "    ## Convert airport_fee into Float\n",
    "    valid_trips['airport_fee'] = pd.to_numeric(valid_trips['airport_fee'], errors='coerce').fillna(0)\n",
    "    \n",
    "    ## Rename: extra -> Miscellaneous_Extras, tpep_pickup_datetime → pickup_datetime, tpep_dropoff_datetime → dropoff_datetime\n",
    "    valid_trips = valid_trips.rename(\n",
    "        columns={'extra':'Miscellaneous_Extras','tpep_pickup_datetime':'pickup_datetime','tpep_dropoff_datetime':'dropoff_datetime'})\n",
    "    \n",
    "    ## Delete records that dropoff_datetime is earlier than pickup_datetime.\n",
    "    valid_trips = valid_trips[valid_trips['dropoff_datetime'] >= valid_trips['pickup_datetime']]\n",
    "    \n",
    "    ## Sampling & Save to parquet file\n",
    "    valid_trips = valid_trips.sample(n=stable_size_yellow, random_state=42).reset_index(drop=True)\n",
    "    valid_trips.to_parquet(output_directory_yellow + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_sampled_records = pd.DataFrame()\n",
    "yellow_sampled_file_names = [f for f in os.listdir(output_directory_yellow) if os.path.isfile(os.path.join(output_directory_yellow, f))]\n",
    "\n",
    "for file in yellow_sampled_file_names:\n",
    "    sampled_df = pd.read_parquet(output_directory_yellow + file)\n",
    "    yellow_sampled_records = pd.concat([yellow_sampled_records,sampled_df],axis=0)\n",
    "    \n",
    "output_directory_final = \"Clean_Sampled_Dataset/Final/\"\n",
    "yellow_sampled_records.to_parquet(output_directory_final + 'Yellow_all.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uber Sampling & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_uber = \"Clean_Sampled_Dataset/Uber/\"\n",
    "os.makedirs(output_directory_uber, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(fhvhv_file_names[:]):\n",
    "    trips_df = pd.read_parquet(fhvhv_file_folder + file)\n",
    "    print('Current Processing:', i, file)\n",
    "    \n",
    "    ## Retain records that are Uber rides\n",
    "    trips_df = trips_df[trips_df['hvfhs_license_num'] == 'HV0003']\n",
    "\n",
    "    # Merge trip data with zone centroids for pickups and dropoffs\n",
    "    trips_with_pickup = trips_df.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='PULocationID',\n",
    "        right_on='LocationID'\n",
    "    ).rename(columns={'latitude': 'pickup_latitude', 'longitude': 'pickup_longitude'})\n",
    "\n",
    "    trips_with_locations = trips_with_pickup.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='DOLocationID',\n",
    "        right_on='LocationID',\n",
    "        suffixes=('', '_dropoff')\n",
    "    ).rename(columns={'latitude': 'dropoff_latitude', 'longitude': 'dropoff_longitude'})\n",
    "\n",
    "    # Filter out trips with invalid location IDs\n",
    "    valid_trips = trips_with_locations.dropna(subset=['pickup_latitude', 'dropoff_latitude'])\n",
    "\n",
    "    # Delete records that start_pos or end_pos is out of range\n",
    "    LAT_MIN, LAT_MAX = 40.560445, 40.908524\n",
    "    LON_MIN, LON_MAX = -74.242330, -73.717047\n",
    "    \n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['pickup_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['pickup_longitude'].between(LON_MIN, LON_MAX)) &\n",
    "        (valid_trips['dropoff_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['dropoff_longitude'].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "    ## Delete original locationID columns\n",
    "    valid_trips.drop(['PULocationID','DOLocationID','LocationID','LocationID_dropoff'],axis=1,inplace=True)\n",
    "    \n",
    "    valid_trips.columns = valid_trips.columns.str.lower()\n",
    "    \n",
    "    ## Delete records that trip_distance is missing or trip_distance <= 0, and convert datatype into Float\n",
    "    valid_trips = valid_trips.dropna(subset=['trip_miles'])\n",
    "    valid_trips = valid_trips[valid_trips['trip_miles']>0]\n",
    "    valid_trips['trip_miles'] = valid_trips['trip_miles'].astype(float)\n",
    "    \n",
    "    ## Delete records that trip_time is missing or trip_distance <= 0, and convert datatype into Float\n",
    "    valid_trips = valid_trips.dropna(subset=['trip_time'])\n",
    "    valid_trips = valid_trips[valid_trips['trip_time']>0]\n",
    "    valid_trips['trip_time'] = valid_trips['trip_time'].astype(float)\n",
    "    \n",
    "    ## Delete records that where base_passenger_fare, tolls, sales_tax, bcf, tips, congestion_surcharge or driver_pay are negative.\n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['base_passenger_fare']>=0) &\n",
    "        (valid_trips['tolls']>=0) &\n",
    "        (valid_trips['sales_tax']>=0) &\n",
    "        (valid_trips['bcf']>=0) &\n",
    "        (valid_trips['tips']>=0) &\n",
    "        (valid_trips['congestion_surcharge']>=0) &\n",
    "        (valid_trips['driver_pay']>=0) \n",
    "        ]\n",
    "    \n",
    "    ## Convert shared_request_flag, shared_match_flag, access_a_ride_flag, wav_request_flag, wav_match_flag into 0 and 1\n",
    "    valid_trips['shared_request_flag'] = valid_trips['shared_request_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    valid_trips['shared_match_flag'] = valid_trips['shared_match_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    valid_trips['access_a_ride_flag'] = valid_trips['access_a_ride_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    valid_trips['wav_request_flag'] = valid_trips['wav_request_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    valid_trips['wav_match_flag'] = valid_trips['wav_match_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    \n",
    "    ## Delete records that dropoff_datetime is earlier than pickup_datetime.\n",
    "    valid_trips = valid_trips[valid_trips['dropoff_datetime'] >= valid_trips['pickup_datetime']]\n",
    "    \n",
    "    ## Delete records that on_scene_datetime is earlier than request_datetime.\n",
    "    valid_trips = valid_trips[valid_trips['on_scene_datetime'] >= valid_trips['request_datetime']]\n",
    "    \n",
    "    ## Rename: bcf -> Black_Car_Fund\n",
    "    valid_trips = valid_trips.rename(\n",
    "        columns={'bcf':'Black_Car_Fund',})\n",
    "    \n",
    "    ## Delete useless columns: dispatching_base_num, Hvfhs_license_num, originating_base_num\n",
    "    valid_trips = valid_trips.drop(['hvfhs_license_num','dispatching_base_num','originating_base_num'], axis=1)\n",
    "    \n",
    "    ## Sampling & Save to parquet file\n",
    "    valid_trips = valid_trips.sample(n=stable_size_uber, random_state=42).reset_index(drop=True)\n",
    "    valid_trips.to_parquet(output_directory_uber + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_sampled_records = pd.DataFrame()\n",
    "uber_sampled_file_names = [f for f in os.listdir(output_directory_uber) if os.path.isfile(os.path.join(output_directory_uber, f))]\n",
    "\n",
    "for file in uber_sampled_file_names:\n",
    "    sampled_df = pd.read_parquet(output_directory_uber + file)\n",
    "    uber_sampled_records = pd.concat([uber_sampled_records,sampled_df],axis=0)\n",
    "    \n",
    "output_directory_final = \"Clean_Sampled_Dataset/Final/\"\n",
    "uber_sampled_records.to_parquet(output_directory_final + 'Uber_all.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data preprocessing: Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Processing: 2020_weather.csv\n",
      "Current Processing: 2021_weather.csv\n",
      "Current Processing: 2022_weather.csv\n",
      "Current Processing: 2023_weather.csv\n",
      "Current Processing: 2024_weather.csv\n"
     ]
    }
   ],
   "source": [
    "weather_data_file_folder = 'Dataset/weather_data/'\n",
    "weather_data_files = ['2020_weather.csv','2021_weather.csv','2022_weather.csv','2023_weather.csv','2024_weather.csv']\n",
    "\n",
    "weather_data_hourly_df = pd.DataFrame()\n",
    "for file in weather_data_files:\n",
    "    print(\"Current Processing:\", file)\n",
    "    weather_data = pd.read_csv(weather_data_file_folder + file)\n",
    "\n",
    "    ## Split Date and Hour\n",
    "    weather_data[['Date', 'Hour']] = weather_data['DATE'].str.split('T', expand=True)\n",
    "    \n",
    "    weather_data_hourly = weather_data[weather_data['Hour'] !='23:59:00']\n",
    "    weather_data_hourly['Hour'] = weather_data_hourly['Hour'].str.split(':').str[0]\n",
    "    weather_data_hourly = weather_data_hourly.groupby(['Date','Hour']).first().reset_index()\n",
    "\n",
    "    original_columns = list(weather_data_hourly.columns)\n",
    "    columns_to_drop = ['DATE','ELEVATION','STATION','NAME','LATITUDE','LONGITUDE','NormalsCoolingDegreeDay','NormalsHeatingDegreeDay','Sunrise', 'Sunset','WindEquipmentChangeDate'] \\\n",
    "    + ['AWND','CDSD','CLDD','DSNW','HDSD','HTDD','DYTS','DYHF',] \\\n",
    "    + ['HourlyPresentWeatherType','HourlySkyConditions','REM','HourlyWindDirection'] \\\n",
    "    + [col for col in original_columns if col.startswith('Daily')] \\\n",
    "    + [col for col in original_columns if col.startswith('Monthly')] \\\n",
    "    + [col for col in original_columns if col.startswith('Backup')] \\\n",
    "    + [col for col in original_columns if col.startswith('ShortDuration')]\n",
    "    weather_data_hourly = weather_data_hourly.drop(columns_to_drop,axis=1)\n",
    "\n",
    "    ## Transform data type & Fill missing values\n",
    "    weather_data_hourly['Hour'] = weather_data_hourly['Hour'].astype(int)\n",
    "    weather_data_hourly['HourlyAltimeterSetting'] = pd.to_numeric(weather_data_hourly['HourlyAltimeterSetting'], errors='coerce')\n",
    "    weather_data_hourly['HourlyAltimeterSetting'] = weather_data_hourly['HourlyAltimeterSetting'].fillna(weather_data_hourly['HourlyAltimeterSetting'].mean())\n",
    "    weather_data_hourly['HourlyDewPointTemperature'] = pd.to_numeric(weather_data_hourly['HourlyDewPointTemperature'], errors='coerce')\n",
    "    weather_data_hourly['HourlyDewPointTemperature'] = weather_data_hourly['HourlyDewPointTemperature'].fillna(weather_data_hourly['HourlyDewPointTemperature'].mean())\n",
    "    weather_data_hourly['HourlyDryBulbTemperature'] = pd.to_numeric(weather_data_hourly['HourlyDryBulbTemperature'], errors='coerce')\n",
    "    weather_data_hourly['HourlyDryBulbTemperature'] = weather_data_hourly['HourlyDryBulbTemperature'].fillna(weather_data_hourly['HourlyDryBulbTemperature'].mean())\n",
    "    weather_data_hourly['HourlySeaLevelPressure'] = pd.to_numeric(weather_data_hourly['HourlySeaLevelPressure'], errors='coerce')\n",
    "    weather_data_hourly['HourlySeaLevelPressure'] = weather_data_hourly['HourlySeaLevelPressure'].fillna(weather_data_hourly['HourlySeaLevelPressure'].mean())\n",
    "    weather_data_hourly['HourlyStationPressure'] = pd.to_numeric(weather_data_hourly['HourlyStationPressure'], errors='coerce')\n",
    "    weather_data_hourly['HourlyStationPressure'] = weather_data_hourly['HourlyStationPressure'].fillna(weather_data_hourly['HourlyStationPressure'].mean())\n",
    "\n",
    "    weather_data_hourly['HourlyVisibility'] = weather_data_hourly['HourlyVisibility'].str.extract(r'(\\d+\\.\\d+)', expand=False)\n",
    "    weather_data_hourly['HourlyVisibility'] = pd.to_numeric(weather_data_hourly['HourlyVisibility'], errors='coerce')\n",
    "    weather_data_hourly['HourlyVisibility'] = weather_data_hourly['HourlyVisibility'].fillna(weather_data_hourly['HourlyVisibility'].mean())\n",
    "\n",
    "    weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].replace('T', 0.0005)\n",
    "    weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].replace(['M', ''], np.nan)\n",
    "    weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].fillna(0)\n",
    "    weather_data_hourly['HourlyPrecipitation'] = pd.to_numeric(weather_data_hourly['HourlyPrecipitation'], errors='coerce')\n",
    "    weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].fillna(0)\n",
    "\n",
    "    weather_data_hourly['HourlyPressureChange'] = pd.to_numeric(weather_data_hourly['HourlyPressureChange'], errors='coerce')\n",
    "    weather_data_hourly['HourlyPressureChange'] = weather_data_hourly['HourlyPressureChange'].fillna(0)\n",
    "    weather_data_hourly['HourlyPressureTendency'] = pd.to_numeric(weather_data_hourly['HourlyPressureTendency'], errors='coerce')\n",
    "    weather_data_hourly['HourlyPressureTendency'] = weather_data_hourly['HourlyPressureTendency'].fillna(0)\n",
    "    weather_data_hourly['HourlyRelativeHumidity'] = pd.to_numeric(weather_data_hourly['HourlyPrecipitation'], errors='coerce')\n",
    "    weather_data_hourly['HourlyRelativeHumidity'] = weather_data_hourly['HourlyRelativeHumidity'].fillna(0)\n",
    "    weather_data_hourly['HourlyWetBulbTemperature'] = pd.to_numeric(weather_data_hourly['HourlyWetBulbTemperature'], errors='coerce')\n",
    "    weather_data_hourly['HourlyWetBulbTemperature'] = weather_data_hourly['HourlyWetBulbTemperature'].fillna(weather_data_hourly['HourlyWetBulbTemperature'].mean())\n",
    "    weather_data_hourly['HourlyWindGustSpeed'] = weather_data_hourly['HourlyWindGustSpeed'].fillna(weather_data_hourly['HourlyWindGustSpeed'].mean())\n",
    "    weather_data_hourly['HourlyWindSpeed'] = weather_data_hourly['HourlyWindSpeed'].fillna(weather_data_hourly['HourlyWindSpeed'].mean())\n",
    "\n",
    "    weather_data_hourly_df = pd.concat([weather_data_hourly_df,weather_data_hourly],axis=0)\n",
    "    \n",
    "output_directory_final = \"Clean_Sampled_Dataset/Final/\"\n",
    "weather_data_hourly_df.to_parquet(output_directory_final + 'Weather_hourly.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>REPORT_TYPE</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>HourlyAltimeterSetting</th>\n",
       "      <th>HourlyDewPointTemperature</th>\n",
       "      <th>HourlyDryBulbTemperature</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyPressureChange</th>\n",
       "      <th>HourlyPressureTendency</th>\n",
       "      <th>HourlyRelativeHumidity</th>\n",
       "      <th>HourlySeaLevelPressure</th>\n",
       "      <th>HourlyStationPressure</th>\n",
       "      <th>HourlyVisibility</th>\n",
       "      <th>HourlyWetBulbTemperature</th>\n",
       "      <th>HourlyWindGustSpeed</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.66</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.64</td>\n",
       "      <td>29.49</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.300203</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.67</td>\n",
       "      <td>27.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.65</td>\n",
       "      <td>29.50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.68</td>\n",
       "      <td>26.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.66</td>\n",
       "      <td>29.51</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.70</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.67</td>\n",
       "      <td>29.53</td>\n",
       "      <td>10.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.70</td>\n",
       "      <td>23.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.67</td>\n",
       "      <td>29.53</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour REPORT_TYPE  SOURCE  HourlyAltimeterSetting  \\\n",
       "0  2020-01-01     0       FM-15       7                   29.66   \n",
       "1  2020-01-01     1       FM-15       7                   29.67   \n",
       "2  2020-01-01     2       FM-15       7                   29.68   \n",
       "3  2020-01-01     3       FM-15       7                   29.70   \n",
       "4  2020-01-01     4       FM-15       7                   29.70   \n",
       "\n",
       "   HourlyDewPointTemperature  HourlyDryBulbTemperature  HourlyPrecipitation  \\\n",
       "0                       26.0                      40.0                  0.0   \n",
       "1                       27.0                      39.0                  0.0   \n",
       "2                       26.0                      39.0                  0.0   \n",
       "3                       24.0                      39.0                  0.0   \n",
       "4                       23.0                      38.0                  0.0   \n",
       "\n",
       "   HourlyPressureChange  HourlyPressureTendency  HourlyRelativeHumidity  \\\n",
       "0                 -0.01                     3.0                     0.0   \n",
       "1                  0.00                     0.0                     0.0   \n",
       "2                  0.00                     0.0                     0.0   \n",
       "3                 -0.03                     3.0                     0.0   \n",
       "4                  0.00                     0.0                     0.0   \n",
       "\n",
       "   HourlySeaLevelPressure  HourlyStationPressure  HourlyVisibility  \\\n",
       "0                   29.64                  29.49              10.0   \n",
       "1                   29.65                  29.50              10.0   \n",
       "2                   29.66                  29.51              10.0   \n",
       "3                   29.67                  29.53              10.0   \n",
       "4                   29.67                  29.53              10.0   \n",
       "\n",
       "   HourlyWetBulbTemperature  HourlyWindGustSpeed  HourlyWindSpeed  \n",
       "0                      35.0            21.300203              8.0  \n",
       "1                      34.0            17.000000              8.0  \n",
       "2                      34.0            23.000000             14.0  \n",
       "3                      33.0            23.000000             11.0  \n",
       "4                      32.0            20.000000              6.0  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data_hourly_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data preprocessing: Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Processing: 2020_weather.csv\n",
      "Current Processing: 2021_weather.csv\n",
      "Current Processing: 2022_weather.csv\n",
      "Current Processing: 2023_weather.csv\n",
      "Current Processing: 2024_weather.csv\n"
     ]
    }
   ],
   "source": [
    "weather_data_file_folder = 'Dataset/weather_data/'\n",
    "weather_data_files = ['2020_weather.csv','2021_weather.csv','2022_weather.csv','2023_weather.csv','2024_weather.csv']\n",
    "\n",
    "weather_data_daily_df = pd.DataFrame()\n",
    "for file in weather_data_files:\n",
    "    print(\"Current Processing:\", file)\n",
    "    weather_data = pd.read_csv(weather_data_file_folder + '2020_weather.csv')\n",
    "\n",
    "    ## Split Date and Hour\n",
    "    weather_data[['Date', 'Hour']] = weather_data['DATE'].str.split('T', expand=True)\n",
    "\n",
    "    weather_data_daily = weather_data[weather_data['Hour'] =='23:59:00']\n",
    "    weather_data_daily['Hour'] = weather_data_daily['Hour'].str.split(':').str[0]\n",
    "    weather_data_daily = weather_data_daily[weather_data_daily['REPORT_TYPE'] == 'SOD  ']\n",
    "    weather_data_daily = weather_data_daily[['Date','Hour'] + [col for col in list(weather_data_daily.columns) if col not in ['Date','Hour']]]\n",
    "    weather_data_daily\n",
    "\n",
    "    original_columns = list(weather_data_daily.columns)\n",
    "    columns_to_drop = ['DATE','ELEVATION','Hour','STATION','NAME','LATITUDE','LONGITUDE','WindEquipmentChangeDate','DailyWeather'] \\\n",
    "        + ['AWND','CDSD','CLDD','DSNW','HDSD','HTDD','DYTS','DYHF',] \\\n",
    "        + ['HourlyPresentWeatherType','HourlySkyConditions','REM','NormalsCoolingDegreeDay','NormalsHeatingDegreeDay','DailySustainedWindDirection'] \\\n",
    "        + [col for col in original_columns if col.startswith('Hourly')] \\\n",
    "        + [col for col in original_columns if col.startswith('Monthly')] \\\n",
    "        + [col for col in original_columns if col.startswith('Backup')] \\\n",
    "        + [col for col in original_columns if col.startswith('ShortDuration')]\n",
    "    weather_data_daily = weather_data_daily.drop(columns_to_drop,axis=1)\n",
    "\n",
    "    special_treat_col1 = ['DailySnowfall','DailyPrecipitation','DailySnowDepth']\n",
    "    for col in special_treat_col1:\n",
    "        weather_data_daily[col] = weather_data_daily[col].replace('T', 0.0005)\n",
    "        weather_data_daily[col] = weather_data_daily[col].replace(['M', ''], np.nan)\n",
    "        weather_data_daily[col] = weather_data_daily[col].fillna(0)\n",
    "        weather_data_daily[col] = pd.to_numeric(weather_data_daily[col], errors='coerce')\n",
    "        weather_data_daily[col] = weather_data_daily[col].fillna(0)\n",
    "        \n",
    "    special_treat_col2 = ['DailyAverageDewPointTemperature','DailyAverageDryBulbTemperature','DailyAverageRelativeHumidity','DailyAverageSeaLevelPressure',\n",
    "    'DailyAverageStationPressure','DailyAverageWetBulbTemperature','DailyAverageWindSpeed',\n",
    "    'DailyCoolingDegreeDays',\n",
    "    'DailyDepartureFromNormalAverageTemperature',\n",
    "    'DailyHeatingDegreeDays',\n",
    "    'DailyCoolingDegreeDays',\n",
    "    'DailyMaximumDryBulbTemperature',\n",
    "    'DailyMinimumDryBulbTemperature',\n",
    "    'DailySustainedWindSpeed',\n",
    "    ]\n",
    "    for col in special_treat_col2:\n",
    "        weather_data_daily[col] = pd.to_numeric(weather_data_daily[col], errors='coerce')\n",
    "        weather_data_daily[col] = weather_data_daily[col].fillna(weather_data_daily[col].mean())\n",
    "\n",
    "    weather_data_daily_df = pd.concat([weather_data_daily_df,weather_data_daily])\n",
    "    \n",
    "output_directory_final = \"Clean_Sampled_Dataset/Final/\"\n",
    "weather_data_daily_df.to_parquet(output_directory_final + 'Weather_daily.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, Float, String, Date, DateTime, MetaData, Table\n",
    "\n",
    "# Create SQLite database\n",
    "engine = create_engine('sqlite:///transport_weather.db')\n",
    "metadata = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['request_datetime',\n",
       " 'on_scene_datetime',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'trip_miles',\n",
       " 'trip_time',\n",
       " 'base_passenger_fare',\n",
       " 'tolls',\n",
       " 'Black_Car_Fund',\n",
       " 'sales_tax',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'tips',\n",
       " 'driver_pay',\n",
       " 'shared_request_flag',\n",
       " 'shared_match_flag',\n",
       " 'access_a_ride_flag',\n",
       " 'wav_request_flag',\n",
       " 'wav_match_flag',\n",
       " 'pickup_latitude',\n",
       " 'pickup_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'dropoff_longitude',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_sampled_records_file = pq.ParquetFile(output_directory_final + 'Uber_all.parquet')\n",
    "uber_sampled_records_file.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Uber trips table schema\n",
    "uber_trips_table = Table(\n",
    "    'uber_trips', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('request_datetime', DateTime),\n",
    "    Column('on_scene_datetime', DateTime),\n",
    "    Column('pickup_datetime', DateTime),\n",
    "    Column('dropoff_datetime', DateTime),\n",
    "    Column('trip_miles', Float),\n",
    "    Column('trip_time', Float),\n",
    "    Column('base_passenger_fare', Float),\n",
    "    Column('tolls', Float),\n",
    "    Column('Black_Car_Fund', Float),\n",
    "    Column('sales_tax', Float),\n",
    "    Column('congestion_surcharge', Float),\n",
    "    Column('airport_fee', Float),\n",
    "    Column('tips', Float),\n",
    "    Column('driver_pay', Float),\n",
    "    Column('shared_request_flag', Integer),  # Y/N\n",
    "    Column('shared_match_flag', Integer),    # Y/N\n",
    "    Column('access_a_ride_flag', Integer),   # Y/N\n",
    "    Column('wav_request_flag', Integer),     # Y/N\n",
    "    Column('wav_match_flag', Integer),       # Y/N\n",
    "    Column('pickup_latitude', Float),\n",
    "    Column('pickup_longitude', Float),\n",
    "    Column('dropoff_latitude', Float),\n",
    "    Column('dropoff_longitude', Float),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vendorid',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RateCodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'Miscellaneous_Extras',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'pickup_latitude',\n",
       " 'pickup_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'dropoff_longitude',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_sampled_records_file = pq.ParquetFile(output_directory_final + 'Yellow_all.parquet')\n",
    "yellow_sampled_records_file.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Yellow Taxi trips table schema\n",
    "yellow_trips_table = Table(\n",
    "    'yellow_taxi_trips', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('vendorid', Integer),  # Can be TEXT or INTEGER depending on data\n",
    "    Column('pickup_datetime', DateTime),\n",
    "    Column('dropoff_datetime', DateTime),\n",
    "    Column('passenger_count', Integer),\n",
    "    Column('trip_distance', Float),\n",
    "    Column('RateCodeID', Integer),\n",
    "    Column('store_and_fwd_flag', Integer),  # Y/N\n",
    "    Column('payment_type', Integer),\n",
    "    Column('fare_amount', Float),\n",
    "    Column('Miscellaneous_Extras', Float),\n",
    "    Column('mta_tax', Float),\n",
    "    Column('tip_amount', Float),\n",
    "    Column('tolls_amount', Float),\n",
    "    Column('improvement_surcharge', Float),\n",
    "    Column('total_amount', Float),\n",
    "    Column('congestion_surcharge', Float),\n",
    "    Column('airport_fee', Float),\n",
    "    Column('pickup_latitude', Float),\n",
    "    Column('pickup_longitude', Float),\n",
    "    Column('dropoff_latitude', Float),\n",
    "    Column('dropoff_longitude', Float),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'Hour',\n",
       " 'REPORT_TYPE',\n",
       " 'SOURCE',\n",
       " 'HourlyAltimeterSetting',\n",
       " 'HourlyDewPointTemperature',\n",
       " 'HourlyDryBulbTemperature',\n",
       " 'HourlyPrecipitation',\n",
       " 'HourlyPressureChange',\n",
       " 'HourlyPressureTendency',\n",
       " 'HourlyRelativeHumidity',\n",
       " 'HourlySeaLevelPressure',\n",
       " 'HourlyStationPressure',\n",
       " 'HourlyVisibility',\n",
       " 'HourlyWetBulbTemperature',\n",
       " 'HourlyWindGustSpeed',\n",
       " 'HourlyWindSpeed',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_hourly_records_file = pq.ParquetFile(output_directory_final + 'Weather_hourly.parquet')\n",
    "weather_hourly_records_file.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weather hourly table schema\n",
    "weather_hourly_table = Table(\n",
    "    'weather_hourly', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('Date', Date),\n",
    "    Column('Hour', Integer),\n",
    "    Column('REPORT_TYPE', String),\n",
    "    Column('SOURCE', String),\n",
    "    Column('HourlyAltimeterSetting', Float),\n",
    "    Column('HourlyDewPointTemperature', Float),\n",
    "    Column('HourlyDryBulbTemperature', Float),\n",
    "    Column('HourlyPrecipitation', Float),\n",
    "    Column('HourlyPressureChange', Float),\n",
    "    Column('HourlyPressureTendency', String),\n",
    "    Column('HourlyRelativeHumidity', Float),\n",
    "    Column('HourlySeaLevelPressure', Float),\n",
    "    Column('HourlyStationPressure', Float),\n",
    "    Column('HourlyVisibility', Float),\n",
    "    Column('HourlyWetBulbTemperature', Float),\n",
    "    Column('HourlyWindGustSpeed', Float),\n",
    "    Column('HourlyWindSpeed', Float),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'REPORT_TYPE',\n",
       " 'SOURCE',\n",
       " 'Sunrise',\n",
       " 'Sunset',\n",
       " 'DailyAverageDewPointTemperature',\n",
       " 'DailyAverageDryBulbTemperature',\n",
       " 'DailyAverageRelativeHumidity',\n",
       " 'DailyAverageSeaLevelPressure',\n",
       " 'DailyAverageStationPressure',\n",
       " 'DailyAverageWetBulbTemperature',\n",
       " 'DailyAverageWindSpeed',\n",
       " 'DailyCoolingDegreeDays',\n",
       " 'DailyDepartureFromNormalAverageTemperature',\n",
       " 'DailyHeatingDegreeDays',\n",
       " 'DailyMaximumDryBulbTemperature',\n",
       " 'DailyMinimumDryBulbTemperature',\n",
       " 'DailyPeakWindDirection',\n",
       " 'DailyPeakWindSpeed',\n",
       " 'DailyPrecipitation',\n",
       " 'DailySnowDepth',\n",
       " 'DailySnowfall',\n",
       " 'DailySustainedWindSpeed',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_daily_records_file = pq.ParquetFile(output_directory_final + 'Weather_daily.parquet')\n",
    "weather_daily_records_file.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weather daily table schema\n",
    "weather_daily_table = Table(\n",
    "    'weather_daily', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('Date', Date),\n",
    "    Column('REPORT_TYPE', String),\n",
    "    Column('SOURCE', String),\n",
    "    Column('Sunrise', String),\n",
    "    Column('Sunset', String),\n",
    "    Column('DailyAverageDewPointTemperature', Float),\n",
    "    Column('DailyAverageDryBulbTemperature', Float),\n",
    "    Column('DailyAverageRelativeHumidity', Float),\n",
    "    Column('DailyAverageSeaLevelPressure', Float),\n",
    "    Column('DailyAverageStationPressure', Float),\n",
    "    Column('DailyAverageWetBulbTemperature', Float),\n",
    "    Column('DailyAverageWindSpeed', Float),\n",
    "    Column('DailyCoolingDegreeDays', Float),\n",
    "    Column('DailyDepartureFromNormalAverageTemperature', Float),\n",
    "    Column('DailyHeatingDegreeDays', Float),\n",
    "    Column('DailyMaximumDryBulbTemperature', Float),\n",
    "    Column('DailyMinimumDryBulbTemperature', Float),\n",
    "    Column('DailyPeakWindDirection', String),\n",
    "    Column('DailyPeakWindSpeed', Float),\n",
    "    Column('DailyPrecipitation', Float),\n",
    "    Column('DailySnowDepth', Float),\n",
    "    Column('DailySnowfall', Float),\n",
    "    Column('DailySustainedWindSpeed', Float),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1830"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df = pd.read_parquet(output_directory_final + 'Yellow_all.parquet')\n",
    "yellow_df.to_sql('yellow_taxi_trips', engine, if_exists='replace', index=False)\n",
    "\n",
    "uber_df = pd.read_parquet(output_directory_final + 'Uber_all.parquet')\n",
    "uber_df.to_sql('uber_trips', engine, if_exists='replace', index=False)\n",
    "\n",
    "weather_hourly_df = pd.read_parquet(output_directory_final + 'Weather_hourly.parquet')\n",
    "weather_hourly_df.to_sql('weather_hourly', engine, if_exists='replace', index=False)\n",
    "\n",
    "weather_daily_df = pd.read_parquet(output_directory_final + 'Weather_daily.parquet')\n",
    "weather_daily_df.to_sql('weather_daily', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date REPORT_TYPE  SOURCE  Sunrise  Sunset  \\\n",
      "0  2020-01-01       SOD         6    720.0  1639.0   \n",
      "1  2020-01-02       SOD         6    720.0  1640.0   \n",
      "2  2020-01-03       SOD         6    720.0  1641.0   \n",
      "3  2020-01-04       SOD         6    720.0  1642.0   \n",
      "4  2020-01-05       SOD         6    720.0  1643.0   \n",
      "\n",
      "   DailyAverageDewPointTemperature  DailyAverageDryBulbTemperature  \\\n",
      "0                             21.0                            38.0   \n",
      "1                             25.0                            41.0   \n",
      "2                             41.0                            47.0   \n",
      "3                             45.0                            46.0   \n",
      "4                             20.0                            39.0   \n",
      "\n",
      "   DailyAverageRelativeHumidity  DailyAverageSeaLevelPressure  \\\n",
      "0                          52.0                         29.76   \n",
      "1                          52.0                         29.91   \n",
      "2                          82.0                         29.81   \n",
      "3                          90.0                         29.62   \n",
      "4                          48.0                         29.84   \n",
      "\n",
      "   DailyAverageStationPressure  ...  \\\n",
      "0                        29.62  ...   \n",
      "1                        29.77  ...   \n",
      "2                        29.67  ...   \n",
      "3                        29.49  ...   \n",
      "4                        29.69  ...   \n",
      "\n",
      "   DailyDepartureFromNormalAverageTemperature  DailyHeatingDegreeDays  \\\n",
      "0                                         4.6                    27.0   \n",
      "1                                         7.7                    24.0   \n",
      "2                                        13.9                    18.0   \n",
      "3                                        13.0                    19.0   \n",
      "4                                         6.1                    26.0   \n",
      "\n",
      "   DailyMaximumDryBulbTemperature  DailyMinimumDryBulbTemperature  \\\n",
      "0                            41.0                            34.0   \n",
      "1                            49.0                            33.0   \n",
      "2                            49.0                            44.0   \n",
      "3                            51.0                            41.0   \n",
      "4                            42.0                            35.0   \n",
      "\n",
      "   DailyPeakWindDirection  DailyPeakWindSpeed  DailyPrecipitation  \\\n",
      "0                   260.0                29.0              0.0000   \n",
      "1                   220.0                22.0              0.0000   \n",
      "2                   230.0                15.0              0.1500   \n",
      "3                   330.0                24.0              0.2700   \n",
      "4                   300.0                43.0              0.0005   \n",
      "\n",
      "   DailySnowDepth  DailySnowfall  DailySustainedWindSpeed  \n",
      "0             0.0            0.0                     17.0  \n",
      "1             0.0            0.0                     13.0  \n",
      "2             0.0            0.0                     10.0  \n",
      "3             0.0            0.0                     15.0  \n",
      "4             0.0            0.0                     25.0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('transport_weather.db')\n",
    "\n",
    "# Query the database\n",
    "query = \"SELECT * FROM weather_daily LIMIT 5\"  # Retrieve first 5 rows\n",
    "result = pd.read_sql(query, conn)\n",
    "\n",
    "print(result)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQL Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema.sql file has been successfully generated.\n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "db_path = 'transport_weather.db' # Replace with your actual database file\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Query the sqlite_master table to get schema definitions\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name, sql FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Open a file to write the schema\n",
    "with open(\"schema.sql\", \"w\") as file:\n",
    "    for table_name, schema in cursor.fetchall():\n",
    "        if schema:  # Exclude views or invalid entries\n",
    "            file.write(schema + \";\\n\\n\")\n",
    "\n",
    "conn.close()\n",
    "print(\"Schema.sql file has been successfully generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
