{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading: programmatically download the Yellow Taxi & High-Volume For-Hire Vehicle (HVFHV) trip data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_URL: str = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "def get_taxi_html() -> str:\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html\n",
    "\n",
    "def find_taxi_parquet_links() -> List[str]:\n",
    "    ### BEGIN SOLUTION\n",
    "    html = get_taxi_html()\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    HVFHV_a_tags = soup.find_all(\"a\", attrs={\"title\": \"High Volume For-Hire Vehicle Trip Records\"})\n",
    "    yellow_a_tags = soup.find_all(\"a\", attrs={\"title\": \"Yellow Taxi Trip Records\"})\n",
    "    all_a_tags = HVFHV_a_tags + yellow_a_tags\n",
    "    return [a[\"href\"] for a in all_a_tags]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_urls(urls, start_year, start_month, end_year, end_month):\n",
    "    filtered_urls = []\n",
    "    for url in urls:\n",
    "        match = re.search(r'(\\d{4})-(\\d{2})', url)\n",
    "        if match:\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            if (start_year < year < end_year) or \\\n",
    "               (year == start_year and month >= start_month) or \\\n",
    "               (year == end_year and month <= end_month):\n",
    "                filtered_urls.append(url.strip())\n",
    "    return filtered_urls\n",
    "\n",
    "# Filtering URLs from January 2020 to August 2024\n",
    "urls = find_taxi_parquet_links()\n",
    "filtered_urls = filter_urls(urls, 2020, 1, 2024, 8)\n",
    "\n",
    "# Display the result\n",
    "urls = sorted(filtered_urls)\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Download all files\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(urls):\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mdownload_parquet_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mdownload_parquet_file\u001b[1;34m(i, url, output_directory)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Save the file\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Downloaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Pytorch\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Pytorch\\Lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Pytorch\\Lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Pytorch\\Lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Pytorch\\Lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Pytorch\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Pytorch\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Pytorch\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Pytorch\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "urls = urls\n",
    "# Directory to save the downloaded files\n",
    "output_directory = \"Dataset\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to download a file\n",
    "def download_parquet_file(i, url, output_directory):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "        \n",
    "        # Extract file name from the URL\n",
    "        file_name = url.split(\"/\")[-1]\n",
    "        file_path = os.path.join(output_directory, file_name)\n",
    "        \n",
    "        # Save the file\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"File {i} Downloaded: {file_name}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"File {i} Failed to download {url}. Error: {e}\")\n",
    "\n",
    "# Download all files\n",
    "for i, url in enumerate(urls):\n",
    "    download_parquet_file(i, url, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6405008, 6299367, 3007687, 238073, 348415, 549797, 800412, 1007286, 1341017, 1681132, 1509000, 1461898, 1369769, 1371709, 1925152, 2171187, 2507109, 2834264, 2821746, 2788757, 2963793, 3463504, 3472949, 3214369, 2463931, 2979431, 3627882, 3599920, 3588295, 3558124, 3174394, 3152677, 3183767, 3675411, 3252717, 3399549, 3066766, 2913955, 3403766, 3288250, 3513649, 3307234, 2907108, 2824209, 2846722, 3522285, 3339715, 3376567, 2964624, 3007526, 3582628, 3514289, 3723833, 3539193, 3076903, 2979183]\n",
      "Yellow Taxi Stable sample size: 664\n",
      "[20569368, 21725100, 13392928, 4312909, 6089999, 7555193, 9958454, 11096852, 12106669, 13268411, 11596865, 11637123, 11908468, 11613942, 14227393, 14111371, 14719171, 14961892, 15027174, 14499696, 14886055, 16545356, 16041639, 16054495, 14751591, 16019283, 18453548, 17752561, 18157335, 17780075, 17464619, 17185687, 17793551, 19306090, 18085896, 19665847, 18479031, 17960971, 20413539, 19144903, 19847676, 19366619, 19132131, 18322150, 19851123, 20186330, 19269250, 20516297, 19663930, 19359148, 21280788, 19733038, 20704538, 20123226, 19182934, 19128392]\n",
      "Uber Stable sample size: 664\n"
     ]
    }
   ],
   "source": [
    "def cochran_sample_size(N, p=0.5, e=0.05, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate Cochran's sample size for a finite population.\n",
    "    \n",
    "    Parameters:\n",
    "    N (int): Population size\n",
    "    p (float): Proportion of population (default=0.5 for max variability)\n",
    "    e (float): Margin of error (default=0.05 for ±5%)\n",
    "    confidence (float): Confidence level (default=0.95 for 95%)\n",
    "    \n",
    "    Returns:\n",
    "    int: Sample size\n",
    "    \"\"\"\n",
    "    # Z-score for the given confidence level\n",
    "    Z = {\n",
    "        0.90: 1.645,\n",
    "        0.95: 1.96,\n",
    "        0.99: 2.576\n",
    "    }.get(confidence, 1.96)  # Default to 95% confidence if not specified\n",
    "    \n",
    "    # Step 1: Cochran's formula for infinite population\n",
    "    n0 = (Z**2 * p * (1 - p)) / (e**2)\n",
    "    \n",
    "    # Step 2: Adjust for finite population\n",
    "    n = n0 / (1 + (n0 - 1) / N)\n",
    "    \n",
    "    return math.ceil(n)\n",
    "\n",
    "def stable_sample_size(monthly_populations, p=0.5, e=0.05, confidence=0.99, method='max'):\n",
    "    \"\"\"\n",
    "    Calculate a stable sample size across multiple months.\n",
    "    \n",
    "    Parameters:\n",
    "    monthly_populations (list): List of population sizes for each month\n",
    "    p (float): Proportion of population (default=0.5 for max variability)\n",
    "    e (float): Margin of error (default=0.05 for ±5%)\n",
    "    confidence (float): Confidence level (default=0.95 for 95%)\n",
    "    method (str): Aggregation method ('max', 'average', 'safety')\n",
    "    \n",
    "    Returns:\n",
    "    int: Stable sample size\n",
    "    \"\"\"\n",
    "    sample_sizes = [cochran_sample_size(N, p, e, confidence) for N in monthly_populations]\n",
    "    print(monthly_populations)\n",
    "    \n",
    "    if method == 'max':\n",
    "        return max(sample_sizes)\n",
    "    elif method == 'average':\n",
    "        return math.ceil(sum(sample_sizes) / len(sample_sizes))\n",
    "    elif method == 'safety':\n",
    "        return math.ceil(max(sample_sizes) * 1.1)  # Add a 10% safety margin\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'max', 'average', or 'safety'.\")\n",
    "\n",
    "## Yellow Taxi\n",
    "yellow_file_folder = 'Dataset/yellow_tripdata/'\n",
    "yellow_file_names = [f for f in os.listdir(yellow_file_folder) if os.path.isfile(os.path.join(yellow_file_folder, f))]\n",
    "\n",
    "monthly_populations_yellow = []\n",
    "for file in yellow_file_names:\n",
    "    parquet_file = pq.ParquetFile(yellow_file_folder + file)\n",
    "    monthly_populations_yellow.append(parquet_file.metadata.num_rows)\n",
    "    \n",
    "# Example monthly population sizes\n",
    "stable_size_yellow = stable_sample_size(monthly_populations_yellow, method='average')\n",
    "print(f\"Yellow Taxi Stable sample size: {stable_size_yellow}\")\n",
    "\n",
    "## Uber\n",
    "fhvhv_file_folder = 'Dataset/fhvhv_tripdata/'\n",
    "fhvhv_file_names = [f for f in os.listdir(fhvhv_file_folder) if os.path.isfile(os.path.join(fhvhv_file_folder, f))]\n",
    "\n",
    "monthly_populations_uber = []\n",
    "for file in fhvhv_file_names:\n",
    "    parquet_file = pq.ParquetFile(fhvhv_file_folder + file)\n",
    "    monthly_populations_uber.append(parquet_file.metadata.num_rows)\n",
    "    \n",
    "# Example monthly population sizes\n",
    "stable_size_uber = stable_sample_size(monthly_populations_uber, method='average')\n",
    "print(f\"Uber Stable sample size: {stable_size_uber}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = 'taxi_zones.shp'\n",
    "zones_gdf = gpd.read_file(shapefile_path)\n",
    "if zones_gdf.crs is None:\n",
    "    zones_gdf.set_crs(epsg=2263, inplace=True)  # Example: NY State Plane (EPSG:2263)\n",
    "zones_gdf = zones_gdf.to_crs(epsg=4326) # Reproject to WGS84 (Latitude/Longitude)\n",
    "\n",
    "# Calculate centroids for each zone (polygon)\n",
    "zones_gdf['centroid'] = zones_gdf.geometry.centroid\n",
    "zones_gdf['latitude'] = zones_gdf['centroid'].y\n",
    "zones_gdf['longitude'] = zones_gdf['centroid'].x\n",
    "\n",
    "# Retain only relevant columns: location ID, latitude, and longitude\n",
    "zones_df = zones_gdf[['LocationID', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow Taxi Filtering and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_yellow = \"Clean_Sampled_Dataset/Yellow/\"\n",
    "os.makedirs(output_directory_yellow, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(yellow_file_names[:]):\n",
    "    trips_df = pd.read_parquet(yellow_file_folder + file)\n",
    "    print('Current Processing:', i, file)\n",
    "\n",
    "    # Merge trip data with zone centroids for pickups and dropoffs\n",
    "    trips_with_pickup = trips_df.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='PULocationID',\n",
    "        right_on='LocationID'\n",
    "    ).rename(columns={'latitude': 'pickup_latitude', 'longitude': 'pickup_longitude'})\n",
    "\n",
    "    trips_with_locations = trips_with_pickup.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='DOLocationID',\n",
    "        right_on='LocationID',\n",
    "        suffixes=('', '_dropoff')\n",
    "    ).rename(columns={'latitude': 'dropoff_latitude', 'longitude': 'dropoff_longitude'})\n",
    "\n",
    "    # Filter out trips with invalid location IDs\n",
    "    valid_trips = trips_with_locations.dropna(subset=['pickup_latitude', 'dropoff_latitude'])\n",
    "\n",
    "    # Delete records that start_pos or end_pos is out of range\n",
    "    LAT_MIN, LAT_MAX = 40.560445, 40.908524\n",
    "    LON_MIN, LON_MAX = -74.242330, -73.717047\n",
    "    \n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['pickup_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['pickup_longitude'].between(LON_MIN, LON_MAX)) &\n",
    "        (valid_trips['dropoff_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['dropoff_longitude'].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "    ## Delete original locationID columns\n",
    "    valid_trips.drop(['PULocationID','DOLocationID','LocationID','LocationID_dropoff'],axis=1,inplace=True)\n",
    "    \n",
    "    valid_trips.columns = valid_trips.columns.str.lower()\n",
    "    \n",
    "    ## Delete records that trip_distance is missing or trip_distance <= 0, and convert datatype into Float\n",
    "    valid_trips = valid_trips.dropna(subset=['trip_distance'])\n",
    "    valid_trips = valid_trips[valid_trips['trip_distance']>0]\n",
    "    valid_trips['trip_distance'] = valid_trips['trip_distance'].astype(float)\n",
    "    \n",
    "    ## Delete records that passenger_count is missing or passenger_count <= 0, and convert datatype into Integer\n",
    "    valid_trips = valid_trips.dropna(subset=['passenger_count'])\n",
    "    valid_trips = valid_trips[valid_trips['passenger_count']>0]\n",
    "    valid_trips['passenger_count'] = valid_trips['passenger_count'].astype(int)\n",
    "    \n",
    "    ## Delete records that where Fare_amount, Total_amount, or Tolls_amount are negative.\n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['fare_amount']>=0) &\n",
    "        (valid_trips['total_amount']>=0) &\n",
    "        (valid_trips['tolls_amount']>=0)\n",
    "        ]\n",
    "    \n",
    "    ## Delete records that Payment_type not in the valid range (1-6), and convert datatype into Integer\n",
    "    valid_trips['payment_type'] = valid_trips['payment_type'].astype(int)\n",
    "    valid_trips = valid_trips[valid_trips['payment_type'].between(1,6)]\n",
    "    \n",
    "    ## Delete records that RateCodeID not in the valid range (1-6), and convert datatype into Integer\n",
    "    valid_trips['ratecodeid'] = valid_trips['ratecodeid'].astype(int)\n",
    "    valid_trips = valid_trips[valid_trips['ratecodeid'].between(1,6)]\n",
    "    valid_trips = valid_trips.rename(columns={'ratecodeid':'RateCodeID',})\n",
    "    \n",
    "    ## Convert store_and_fwd_flag into 0 and 1\n",
    "    valid_trips['store_and_fwd_flag'] = valid_trips['store_and_fwd_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    \n",
    "    ## Convert airport_fee into Float\n",
    "    valid_trips['airport_fee'] = pd.to_numeric(valid_trips['airport_fee'], errors='coerce').fillna(0)\n",
    "    \n",
    "    ## Rename: extra -> Miscellaneous_Extras, tpep_pickup_datetime → pickup_datetime, tpep_dropoff_datetime → dropoff_datetime\n",
    "    valid_trips = valid_trips.rename(\n",
    "        columns={'extra':'Miscellaneous_Extras','tpep_pickup_datetime':'pickup_datetime','tpep_dropoff_datetime':'dropoff_datetime'})\n",
    "    \n",
    "    ## Delete records that dropoff_datetime is earlier than pickup_datetime.\n",
    "    valid_trips = valid_trips[valid_trips['dropoff_datetime'] >= valid_trips['pickup_datetime']]\n",
    "    \n",
    "    ## Sampling & Save to parquet file\n",
    "    valid_trips = valid_trips.sample(n=stable_size_yellow, random_state=42).reset_index(drop=True)\n",
    "    valid_trips.to_parquet(output_directory_yellow + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_sampled_records = pd.DataFrame()\n",
    "yellow_sampled_file_names = [f for f in os.listdir(output_directory_yellow) if os.path.isfile(os.path.join(output_directory_yellow, f))]\n",
    "\n",
    "for file in yellow_sampled_file_names:\n",
    "    sampled_df = pd.read_parquet(output_directory_yellow + file)\n",
    "    yellow_sampled_records = pd.concat([yellow_sampled_records,sampled_df],axis=0)\n",
    "    \n",
    "output_directory_final = \"Clean_Sampled_Dataset/Final/\"\n",
    "yellow_sampled_records.to_parquet(output_directory_final + 'Yellow_all.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uber Sampling & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_uber = \"Clean_Sampled_Dataset/Uber/\"\n",
    "os.makedirs(output_directory_uber, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(fhvhv_file_names[:]):\n",
    "    trips_df = pd.read_parquet(fhvhv_file_folder + file)\n",
    "    print('Current Processing:', i, file)\n",
    "    \n",
    "    ## Retain records that are Uber rides\n",
    "    trips_df = trips_df[trips_df['hvfhs_license_num'] == 'HV0003']\n",
    "\n",
    "    # Merge trip data with zone centroids for pickups and dropoffs\n",
    "    trips_with_pickup = trips_df.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='PULocationID',\n",
    "        right_on='LocationID'\n",
    "    ).rename(columns={'latitude': 'pickup_latitude', 'longitude': 'pickup_longitude'})\n",
    "\n",
    "    trips_with_locations = trips_with_pickup.merge(\n",
    "        zones_df,\n",
    "        how='left',\n",
    "        left_on='DOLocationID',\n",
    "        right_on='LocationID',\n",
    "        suffixes=('', '_dropoff')\n",
    "    ).rename(columns={'latitude': 'dropoff_latitude', 'longitude': 'dropoff_longitude'})\n",
    "\n",
    "    # Filter out trips with invalid location IDs\n",
    "    valid_trips = trips_with_locations.dropna(subset=['pickup_latitude', 'dropoff_latitude'])\n",
    "\n",
    "    # Delete records that start_pos or end_pos is out of range\n",
    "    LAT_MIN, LAT_MAX = 40.560445, 40.908524\n",
    "    LON_MIN, LON_MAX = -74.242330, -73.717047\n",
    "    \n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['pickup_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['pickup_longitude'].between(LON_MIN, LON_MAX)) &\n",
    "        (valid_trips['dropoff_latitude'].between(LAT_MIN, LAT_MAX)) &\n",
    "        (valid_trips['dropoff_longitude'].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "    ## Delete original locationID columns\n",
    "    valid_trips.drop(['PULocationID','DOLocationID','LocationID','LocationID_dropoff'],axis=1,inplace=True)\n",
    "    \n",
    "    valid_trips.columns = valid_trips.columns.str.lower()\n",
    "    \n",
    "    ## Delete records that trip_distance is missing or trip_distance <= 0, and convert datatype into Float\n",
    "    valid_trips = valid_trips.dropna(subset=['trip_miles'])\n",
    "    valid_trips = valid_trips[valid_trips['trip_miles']>0]\n",
    "    valid_trips['trip_miles'] = valid_trips['trip_miles'].astype(float)\n",
    "    \n",
    "    ## Delete records that trip_time is missing or trip_distance <= 0, and convert datatype into Float\n",
    "    valid_trips = valid_trips.dropna(subset=['trip_time'])\n",
    "    valid_trips = valid_trips[valid_trips['trip_time']>0]\n",
    "    valid_trips['trip_time'] = valid_trips['trip_time'].astype(float)\n",
    "    \n",
    "    ## Delete records that where base_passenger_fare, tolls, sales_tax, bcf, tips, congestion_surcharge or driver_pay are negative.\n",
    "    valid_trips = valid_trips[\n",
    "        (valid_trips['base_passenger_fare']>=0) &\n",
    "        (valid_trips['tolls']>=0) &\n",
    "        (valid_trips['sales_tax']>=0) &\n",
    "        (valid_trips['bcf']>=0) &\n",
    "        (valid_trips['tips']>=0) &\n",
    "        (valid_trips['congestion_surcharge']>=0) &\n",
    "        (valid_trips['driver_pay']>=0) \n",
    "        ]\n",
    "    \n",
    "    ## Convert shared_request_flag, shared_match_flag, access_a_ride_flag, wav_request_flag, wav_match_flag into 0 and 1\n",
    "    valid_trips['shared_request_flag'] = valid_trips['shared_request_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    valid_trips['shared_match_flag'] = valid_trips['shared_match_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    valid_trips['access_a_ride_flag'] = valid_trips['access_a_ride_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    valid_trips['wav_request_flag'] = valid_trips['wav_request_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    valid_trips['wav_match_flag'] = valid_trips['wav_match_flag'].map({'Y':1,'N':0}).fillna(0)\n",
    "    \n",
    "    ## Delete records that dropoff_datetime is earlier than pickup_datetime.\n",
    "    valid_trips = valid_trips[valid_trips['dropoff_datetime'] >= valid_trips['pickup_datetime']]\n",
    "    \n",
    "    ## Delete records that on_scene_datetime is earlier than request_datetime.\n",
    "    valid_trips = valid_trips[valid_trips['on_scene_datetime'] >= valid_trips['request_datetime']]\n",
    "    \n",
    "    ## Rename: bcf -> Black_Car_Fund\n",
    "    valid_trips = valid_trips.rename(\n",
    "        columns={'bcf':'Black_Car_Fund',})\n",
    "    \n",
    "    ## Delete useless columns: dispatching_base_num, Hvfhs_license_num, originating_base_num\n",
    "    valid_trips = valid_trips.drop(['hvfhs_license_num','dispatching_base_num','originating_base_num'], axis=1)\n",
    "    \n",
    "    ## Sampling & Save to parquet file\n",
    "    valid_trips = valid_trips.sample(n=stable_size_uber, random_state=42).reset_index(drop=True)\n",
    "    valid_trips.to_parquet(output_directory_uber + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_sampled_records = pd.DataFrame()\n",
    "uber_sampled_file_names = [f for f in os.listdir(output_directory_uber) if os.path.isfile(os.path.join(output_directory_uber, f))]\n",
    "\n",
    "for file in uber_sampled_file_names:\n",
    "    sampled_df = pd.read_parquet(output_directory_uber + file)\n",
    "    uber_sampled_records = pd.concat([uber_sampled_records,sampled_df],axis=0)\n",
    "    \n",
    "output_directory_final = \"Clean_Sampled_Dataset/Final/\"\n",
    "uber_sampled_records.to_parquet(output_directory_final + 'Uber_all.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data preprocessing: Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Processing: 2020_weather.csv\n",
      "Current Processing: 2021_weather.csv\n",
      "Current Processing: 2022_weather.csv\n",
      "Current Processing: 2023_weather.csv\n",
      "Current Processing: 2024_weather.csv\n"
     ]
    }
   ],
   "source": [
    "weather_data_file_folder = 'Dataset/weather_data/'\n",
    "weather_data_files = ['2020_weather.csv','2021_weather.csv','2022_weather.csv','2023_weather.csv','2024_weather.csv']\n",
    "\n",
    "weather_data_hourly_df = pd.DataFrame()\n",
    "for file in weather_data_files:\n",
    "    print(\"Current Processing:\", file)\n",
    "    weather_data = pd.read_csv(weather_data_file_folder + file)\n",
    "\n",
    "    ## Split Date and Hour\n",
    "    weather_data[['Date', 'Hour']] = weather_data['DATE'].str.split('T', expand=True)\n",
    "    \n",
    "    weather_data_hourly = weather_data[weather_data['Hour'] !='23:59:00']\n",
    "    weather_data_hourly['Hour'] = weather_data_hourly['Hour'].str.split(':').str[0]\n",
    "    weather_data_hourly = weather_data_hourly.groupby(['Date','Hour']).first().reset_index()\n",
    "\n",
    "    original_columns = list(weather_data_hourly.columns)\n",
    "    columns_to_drop = ['DATE','ELEVATION','STATION','NAME','LATITUDE','LONGITUDE','NormalsCoolingDegreeDay','NormalsHeatingDegreeDay','Sunrise', 'Sunset','WindEquipmentChangeDate'] \\\n",
    "    + ['AWND','CDSD','CLDD','DSNW','HDSD','HTDD','DYTS','DYHF',] \\\n",
    "    + ['HourlyPresentWeatherType','HourlySkyConditions','REM','HourlyWindDirection'] \\\n",
    "    + [col for col in original_columns if col.startswith('Daily')] \\\n",
    "    + [col for col in original_columns if col.startswith('Monthly')] \\\n",
    "    + [col for col in original_columns if col.startswith('Backup')] \\\n",
    "    + [col for col in original_columns if col.startswith('ShortDuration')]\n",
    "    weather_data_hourly = weather_data_hourly.drop(columns_to_drop,axis=1)\n",
    "\n",
    "    ## Transform data type & Fill missing values\n",
    "    weather_data_hourly['Hour'] = weather_data_hourly['Hour'].astype(int)\n",
    "    weather_data_hourly['HourlyAltimeterSetting'] = pd.to_numeric(weather_data_hourly['HourlyAltimeterSetting'], errors='coerce')\n",
    "    weather_data_hourly['HourlyAltimeterSetting'] = weather_data_hourly['HourlyAltimeterSetting'].fillna(weather_data_hourly['HourlyAltimeterSetting'].mean())\n",
    "    weather_data_hourly['HourlyDewPointTemperature'] = pd.to_numeric(weather_data_hourly['HourlyDewPointTemperature'], errors='coerce')\n",
    "    weather_data_hourly['HourlyDewPointTemperature'] = weather_data_hourly['HourlyDewPointTemperature'].fillna(weather_data_hourly['HourlyDewPointTemperature'].mean())\n",
    "    weather_data_hourly['HourlyDryBulbTemperature'] = pd.to_numeric(weather_data_hourly['HourlyDryBulbTemperature'], errors='coerce')\n",
    "    weather_data_hourly['HourlyDryBulbTemperature'] = weather_data_hourly['HourlyDryBulbTemperature'].fillna(weather_data_hourly['HourlyDryBulbTemperature'].mean())\n",
    "    weather_data_hourly['HourlySeaLevelPressure'] = pd.to_numeric(weather_data_hourly['HourlySeaLevelPressure'], errors='coerce')\n",
    "    weather_data_hourly['HourlySeaLevelPressure'] = weather_data_hourly['HourlySeaLevelPressure'].fillna(weather_data_hourly['HourlySeaLevelPressure'].mean())\n",
    "    weather_data_hourly['HourlyStationPressure'] = pd.to_numeric(weather_data_hourly['HourlyStationPressure'], errors='coerce')\n",
    "    weather_data_hourly['HourlyStationPressure'] = weather_data_hourly['HourlyStationPressure'].fillna(weather_data_hourly['HourlyStationPressure'].mean())\n",
    "\n",
    "    weather_data_hourly['HourlyVisibility'] = weather_data_hourly['HourlyVisibility'].str.extract(r'(\\d+\\.\\d+)', expand=False)\n",
    "    weather_data_hourly['HourlyVisibility'] = pd.to_numeric(weather_data_hourly['HourlyVisibility'], errors='coerce')\n",
    "    weather_data_hourly['HourlyVisibility'] = weather_data_hourly['HourlyVisibility'].fillna(weather_data_hourly['HourlyVisibility'].mean())\n",
    "\n",
    "    weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].replace('T', 0.0005)\n",
    "    weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].replace(['M', ''], np.nan)\n",
    "    weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].fillna(0)\n",
    "    weather_data_hourly['HourlyPrecipitation'] = pd.to_numeric(weather_data_hourly['HourlyPrecipitation'], errors='coerce')\n",
    "    weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].fillna(0)\n",
    "\n",
    "    weather_data_hourly['HourlyPressureChange'] = pd.to_numeric(weather_data_hourly['HourlyPressureChange'], errors='coerce')\n",
    "    weather_data_hourly['HourlyPressureChange'] = weather_data_hourly['HourlyPressureChange'].fillna(0)\n",
    "    weather_data_hourly['HourlyPressureTendency'] = pd.to_numeric(weather_data_hourly['HourlyPressureTendency'], errors='coerce')\n",
    "    weather_data_hourly['HourlyPressureTendency'] = weather_data_hourly['HourlyPressureTendency'].fillna(0)\n",
    "    weather_data_hourly['HourlyRelativeHumidity'] = pd.to_numeric(weather_data_hourly['HourlyPrecipitation'], errors='coerce')\n",
    "    weather_data_hourly['HourlyRelativeHumidity'] = weather_data_hourly['HourlyRelativeHumidity'].fillna(0)\n",
    "    weather_data_hourly['HourlyWetBulbTemperature'] = pd.to_numeric(weather_data_hourly['HourlyWetBulbTemperature'], errors='coerce')\n",
    "    weather_data_hourly['HourlyWetBulbTemperature'] = weather_data_hourly['HourlyWetBulbTemperature'].fillna(weather_data_hourly['HourlyWetBulbTemperature'].mean())\n",
    "    weather_data_hourly['HourlyWindGustSpeed'] = weather_data_hourly['HourlyWindGustSpeed'].fillna(weather_data_hourly['HourlyWindGustSpeed'].mean())\n",
    "    weather_data_hourly['HourlyWindSpeed'] = weather_data_hourly['HourlyWindSpeed'].fillna(weather_data_hourly['HourlyWindSpeed'].mean())\n",
    "\n",
    "    weather_data_hourly_df = pd.concat([weather_data_hourly_df,weather_data_hourly],axis=0)\n",
    "    \n",
    "output_directory_final = \"Clean_Sampled_Dataset/Final/\"\n",
    "weather_data_hourly_df.to_parquet(output_directory_final + 'Weather_hourly.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>REPORT_TYPE</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>HourlyAltimeterSetting</th>\n",
       "      <th>HourlyDewPointTemperature</th>\n",
       "      <th>HourlyDryBulbTemperature</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyPressureChange</th>\n",
       "      <th>HourlyPressureTendency</th>\n",
       "      <th>HourlyRelativeHumidity</th>\n",
       "      <th>HourlySeaLevelPressure</th>\n",
       "      <th>HourlyStationPressure</th>\n",
       "      <th>HourlyVisibility</th>\n",
       "      <th>HourlyWetBulbTemperature</th>\n",
       "      <th>HourlyWindGustSpeed</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.66</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.64</td>\n",
       "      <td>29.49</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.300203</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.67</td>\n",
       "      <td>27.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.65</td>\n",
       "      <td>29.50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.68</td>\n",
       "      <td>26.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.66</td>\n",
       "      <td>29.51</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.70</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.67</td>\n",
       "      <td>29.53</td>\n",
       "      <td>10.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.70</td>\n",
       "      <td>23.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.67</td>\n",
       "      <td>29.53</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour REPORT_TYPE  SOURCE  HourlyAltimeterSetting  \\\n",
       "0  2020-01-01     0       FM-15       7                   29.66   \n",
       "1  2020-01-01     1       FM-15       7                   29.67   \n",
       "2  2020-01-01     2       FM-15       7                   29.68   \n",
       "3  2020-01-01     3       FM-15       7                   29.70   \n",
       "4  2020-01-01     4       FM-15       7                   29.70   \n",
       "\n",
       "   HourlyDewPointTemperature  HourlyDryBulbTemperature  HourlyPrecipitation  \\\n",
       "0                       26.0                      40.0                  0.0   \n",
       "1                       27.0                      39.0                  0.0   \n",
       "2                       26.0                      39.0                  0.0   \n",
       "3                       24.0                      39.0                  0.0   \n",
       "4                       23.0                      38.0                  0.0   \n",
       "\n",
       "   HourlyPressureChange  HourlyPressureTendency  HourlyRelativeHumidity  \\\n",
       "0                 -0.01                     3.0                     0.0   \n",
       "1                  0.00                     0.0                     0.0   \n",
       "2                  0.00                     0.0                     0.0   \n",
       "3                 -0.03                     3.0                     0.0   \n",
       "4                  0.00                     0.0                     0.0   \n",
       "\n",
       "   HourlySeaLevelPressure  HourlyStationPressure  HourlyVisibility  \\\n",
       "0                   29.64                  29.49              10.0   \n",
       "1                   29.65                  29.50              10.0   \n",
       "2                   29.66                  29.51              10.0   \n",
       "3                   29.67                  29.53              10.0   \n",
       "4                   29.67                  29.53              10.0   \n",
       "\n",
       "   HourlyWetBulbTemperature  HourlyWindGustSpeed  HourlyWindSpeed  \n",
       "0                      35.0            21.300203              8.0  \n",
       "1                      34.0            17.000000              8.0  \n",
       "2                      34.0            23.000000             14.0  \n",
       "3                      33.0            23.000000             11.0  \n",
       "4                      32.0            20.000000              6.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data_hourly_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data preprocessing: Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Processing: 2020_weather.csv\n",
      "Current Processing: 2021_weather.csv\n",
      "Current Processing: 2022_weather.csv\n",
      "Current Processing: 2023_weather.csv\n",
      "Current Processing: 2024_weather.csv\n"
     ]
    }
   ],
   "source": [
    "weather_data_file_folder = 'Dataset/weather_data/'\n",
    "weather_data_files = ['2020_weather.csv','2021_weather.csv','2022_weather.csv','2023_weather.csv','2024_weather.csv']\n",
    "\n",
    "weather_data_daily_df = pd.DataFrame()\n",
    "for file in weather_data_files:\n",
    "    print(\"Current Processing:\", file)\n",
    "    weather_data = pd.read_csv(weather_data_file_folder + file)\n",
    "\n",
    "    ## Split Date and Hour\n",
    "    weather_data[['Date', 'Hour']] = weather_data['DATE'].str.split('T', expand=True)\n",
    "\n",
    "    weather_data_daily = weather_data[weather_data['Hour'] =='23:59:00']\n",
    "    weather_data_daily['Hour'] = weather_data_daily['Hour'].str.split(':').str[0]\n",
    "    weather_data_daily = weather_data_daily[weather_data_daily['REPORT_TYPE'] == 'SOD  ']\n",
    "    weather_data_daily = weather_data_daily[['Date','Hour'] + [col for col in list(weather_data_daily.columns) if col not in ['Date','Hour']]]\n",
    "\n",
    "    original_columns = list(weather_data_daily.columns)\n",
    "    columns_to_drop = ['DATE','ELEVATION','Hour','STATION','NAME','LATITUDE','LONGITUDE','WindEquipmentChangeDate','DailyWeather'] \\\n",
    "        + ['AWND','CDSD','CLDD','DSNW','HDSD','HTDD','DYTS','DYHF',] \\\n",
    "        + ['HourlyPresentWeatherType','HourlySkyConditions','REM','NormalsCoolingDegreeDay','DailyPeakWindDirection','NormalsHeatingDegreeDay','DailySustainedWindDirection'] \\\n",
    "        + [col for col in original_columns if col.startswith('Hourly')] \\\n",
    "        + [col for col in original_columns if col.startswith('Monthly')] \\\n",
    "        + [col for col in original_columns if col.startswith('Backup')] \\\n",
    "        + [col for col in original_columns if col.startswith('ShortDuration')]\n",
    "    weather_data_daily = weather_data_daily.drop(columns_to_drop,axis=1)\n",
    "\n",
    "    special_treat_col1 = ['DailySnowfall','DailyPrecipitation','DailySnowDepth']\n",
    "    for col in special_treat_col1:\n",
    "        weather_data_daily[col] = weather_data_daily[col].replace('T', 0.0005)\n",
    "        weather_data_daily[col] = weather_data_daily[col].replace(['M', ''], np.nan)\n",
    "        weather_data_daily[col] = weather_data_daily[col].fillna(0)\n",
    "        weather_data_daily[col] = pd.to_numeric(weather_data_daily[col], errors='coerce')\n",
    "        weather_data_daily[col] = weather_data_daily[col].fillna(0)\n",
    "        \n",
    "    special_treat_col2 = ['DailyAverageDewPointTemperature','DailyAverageDryBulbTemperature','DailyAverageRelativeHumidity','DailyAverageSeaLevelPressure',\n",
    "    'DailyAverageStationPressure','DailyAverageWetBulbTemperature','DailyAverageWindSpeed',\n",
    "    'DailyCoolingDegreeDays',\n",
    "    'DailyDepartureFromNormalAverageTemperature',\n",
    "    'DailyHeatingDegreeDays',\n",
    "    'DailyCoolingDegreeDays',\n",
    "    'DailyMaximumDryBulbTemperature',\n",
    "    'DailyMinimumDryBulbTemperature',\n",
    "    'DailySustainedWindSpeed',\n",
    "    'DailyPeakWindSpeed',\n",
    "    ]\n",
    "    for col in special_treat_col2:\n",
    "        weather_data_daily[col] = pd.to_numeric(weather_data_daily[col], errors='coerce')\n",
    "        weather_data_daily[col] = weather_data_daily[col].fillna(weather_data_daily[col].mean())\n",
    "\n",
    "    weather_data_daily_df = pd.concat([weather_data_daily_df,weather_data_daily])\n",
    "    \n",
    "output_directory_final = \"Clean_Sampled_Dataset/Final/\"\n",
    "weather_data_daily_df.to_parquet(output_directory_final + 'Weather_daily.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, Float, String, Date, DateTime, MetaData, Table\n",
    "\n",
    "# Create SQLite database\n",
    "engine = create_engine('sqlite:///transport_weather.db')\n",
    "metadata = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['request_datetime',\n",
       " 'on_scene_datetime',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'trip_miles',\n",
       " 'trip_time',\n",
       " 'base_passenger_fare',\n",
       " 'tolls',\n",
       " 'Black_Car_Fund',\n",
       " 'sales_tax',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'tips',\n",
       " 'driver_pay',\n",
       " 'shared_request_flag',\n",
       " 'shared_match_flag',\n",
       " 'access_a_ride_flag',\n",
       " 'wav_request_flag',\n",
       " 'wav_match_flag',\n",
       " 'pickup_latitude',\n",
       " 'pickup_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'dropoff_longitude',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_sampled_records_file = pq.ParquetFile(output_directory_final + 'Uber_all.parquet')\n",
    "uber_sampled_records_file.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Uber trips table schema\n",
    "uber_trips_table = Table(\n",
    "    'uber_trips', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('request_datetime', DateTime),\n",
    "    Column('on_scene_datetime', DateTime),\n",
    "    Column('pickup_datetime', DateTime),\n",
    "    Column('dropoff_datetime', DateTime),\n",
    "    Column('trip_miles', Float),\n",
    "    Column('trip_time', Float),\n",
    "    Column('base_passenger_fare', Float),\n",
    "    Column('tolls', Float),\n",
    "    Column('Black_Car_Fund', Float),\n",
    "    Column('sales_tax', Float),\n",
    "    Column('congestion_surcharge', Float),\n",
    "    Column('airport_fee', Float),\n",
    "    Column('tips', Float),\n",
    "    Column('driver_pay', Float),\n",
    "    Column('shared_request_flag', Integer),  # Y/N\n",
    "    Column('shared_match_flag', Integer),    # Y/N\n",
    "    Column('access_a_ride_flag', Integer),   # Y/N\n",
    "    Column('wav_request_flag', Integer),     # Y/N\n",
    "    Column('wav_match_flag', Integer),       # Y/N\n",
    "    Column('pickup_latitude', Float),\n",
    "    Column('pickup_longitude', Float),\n",
    "    Column('dropoff_latitude', Float),\n",
    "    Column('dropoff_longitude', Float),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vendorid',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RateCodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'Miscellaneous_Extras',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'pickup_latitude',\n",
       " 'pickup_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'dropoff_longitude',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_sampled_records_file = pq.ParquetFile(output_directory_final + 'Yellow_all.parquet')\n",
    "yellow_sampled_records_file.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Yellow Taxi trips table schema\n",
    "yellow_trips_table = Table(\n",
    "    'yellow_taxi_trips', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('vendorid', Integer),  # Can be TEXT or INTEGER depending on data\n",
    "    Column('pickup_datetime', DateTime),\n",
    "    Column('dropoff_datetime', DateTime),\n",
    "    Column('passenger_count', Integer),\n",
    "    Column('trip_distance', Float),\n",
    "    Column('RateCodeID', Integer),\n",
    "    Column('store_and_fwd_flag', Integer),  # Y/N\n",
    "    Column('payment_type', Integer),\n",
    "    Column('fare_amount', Float),\n",
    "    Column('Miscellaneous_Extras', Float),\n",
    "    Column('mta_tax', Float),\n",
    "    Column('tip_amount', Float),\n",
    "    Column('tolls_amount', Float),\n",
    "    Column('improvement_surcharge', Float),\n",
    "    Column('total_amount', Float),\n",
    "    Column('congestion_surcharge', Float),\n",
    "    Column('airport_fee', Float),\n",
    "    Column('pickup_latitude', Float),\n",
    "    Column('pickup_longitude', Float),\n",
    "    Column('dropoff_latitude', Float),\n",
    "    Column('dropoff_longitude', Float),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'Hour',\n",
       " 'REPORT_TYPE',\n",
       " 'SOURCE',\n",
       " 'HourlyAltimeterSetting',\n",
       " 'HourlyDewPointTemperature',\n",
       " 'HourlyDryBulbTemperature',\n",
       " 'HourlyPrecipitation',\n",
       " 'HourlyPressureChange',\n",
       " 'HourlyPressureTendency',\n",
       " 'HourlyRelativeHumidity',\n",
       " 'HourlySeaLevelPressure',\n",
       " 'HourlyStationPressure',\n",
       " 'HourlyVisibility',\n",
       " 'HourlyWetBulbTemperature',\n",
       " 'HourlyWindGustSpeed',\n",
       " 'HourlyWindSpeed',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_hourly_records_file = pq.ParquetFile(output_directory_final + 'Weather_hourly.parquet')\n",
    "weather_hourly_records_file.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weather hourly table schema\n",
    "weather_hourly_table = Table(\n",
    "    'weather_hourly', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('Date', Date),\n",
    "    Column('Hour', Integer),\n",
    "    Column('REPORT_TYPE', String),\n",
    "    Column('SOURCE', String),\n",
    "    Column('HourlyAltimeterSetting', Float),\n",
    "    Column('HourlyDewPointTemperature', Float),\n",
    "    Column('HourlyDryBulbTemperature', Float),\n",
    "    Column('HourlyPrecipitation', Float),\n",
    "    Column('HourlyPressureChange', Float),\n",
    "    Column('HourlyPressureTendency', String),\n",
    "    Column('HourlyRelativeHumidity', Float),\n",
    "    Column('HourlySeaLevelPressure', Float),\n",
    "    Column('HourlyStationPressure', Float),\n",
    "    Column('HourlyVisibility', Float),\n",
    "    Column('HourlyWetBulbTemperature', Float),\n",
    "    Column('HourlyWindGustSpeed', Float),\n",
    "    Column('HourlyWindSpeed', Float),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'REPORT_TYPE',\n",
       " 'SOURCE',\n",
       " 'Sunrise',\n",
       " 'Sunset',\n",
       " 'DailyAverageDewPointTemperature',\n",
       " 'DailyAverageDryBulbTemperature',\n",
       " 'DailyAverageRelativeHumidity',\n",
       " 'DailyAverageSeaLevelPressure',\n",
       " 'DailyAverageStationPressure',\n",
       " 'DailyAverageWetBulbTemperature',\n",
       " 'DailyAverageWindSpeed',\n",
       " 'DailyCoolingDegreeDays',\n",
       " 'DailyDepartureFromNormalAverageTemperature',\n",
       " 'DailyHeatingDegreeDays',\n",
       " 'DailyMaximumDryBulbTemperature',\n",
       " 'DailyMinimumDryBulbTemperature',\n",
       " 'DailyPeakWindSpeed',\n",
       " 'DailyPrecipitation',\n",
       " 'DailySnowDepth',\n",
       " 'DailySnowfall',\n",
       " 'DailySustainedWindSpeed',\n",
       " '__index_level_0__']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_daily_records_file = pq.ParquetFile(output_directory_final + 'Weather_daily.parquet')\n",
    "weather_daily_records_file.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weather daily table schema\n",
    "weather_daily_table = Table(\n",
    "    'weather_daily', metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('Date', Date),\n",
    "    Column('REPORT_TYPE', String),\n",
    "    Column('SOURCE', String),\n",
    "    Column('Sunrise', String),\n",
    "    Column('Sunset', String),\n",
    "    Column('DailyAverageDewPointTemperature', Float),\n",
    "    Column('DailyAverageDryBulbTemperature', Float),\n",
    "    Column('DailyAverageRelativeHumidity', Float),\n",
    "    Column('DailyAverageSeaLevelPressure', Float),\n",
    "    Column('DailyAverageStationPressure', Float),\n",
    "    Column('DailyAverageWetBulbTemperature', Float),\n",
    "    Column('DailyAverageWindSpeed', Float),\n",
    "    Column('DailyCoolingDegreeDays', Float),\n",
    "    Column('DailyDepartureFromNormalAverageTemperature', Float),\n",
    "    Column('DailyHeatingDegreeDays', Float),\n",
    "    Column('DailyMaximumDryBulbTemperature', Float),\n",
    "    Column('DailyMinimumDryBulbTemperature', Float),\n",
    "    Column('DailyPeakWindSpeed', Float),\n",
    "    Column('DailyPrecipitation', Float),\n",
    "    Column('DailySnowDepth', Float),\n",
    "    Column('DailySnowfall', Float),\n",
    "    Column('DailySustainedWindSpeed', Float),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1755"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df = pd.read_parquet(output_directory_final + 'Yellow_all.parquet')\n",
    "yellow_df.to_sql('yellow_taxi_trips', engine, if_exists='replace', index=False)\n",
    "\n",
    "uber_df = pd.read_parquet(output_directory_final + 'Uber_all.parquet')\n",
    "uber_df.to_sql('uber_trips', engine, if_exists='replace', index=False)\n",
    "\n",
    "weather_hourly_df = pd.read_parquet(output_directory_final + 'Weather_hourly.parquet')\n",
    "weather_hourly_df.to_sql('weather_hourly', engine, if_exists='replace', index=False)\n",
    "\n",
    "weather_daily_df = pd.read_parquet(output_directory_final + 'Weather_daily.parquet')\n",
    "weather_daily_df.to_sql('weather_daily', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date REPORT_TYPE  SOURCE  Sunrise  Sunset  \\\n",
      "0  2020-01-01       SOD         6    720.0  1639.0   \n",
      "1  2020-01-02       SOD         6    720.0  1640.0   \n",
      "2  2020-01-03       SOD         6    720.0  1641.0   \n",
      "3  2020-01-04       SOD         6    720.0  1642.0   \n",
      "4  2020-01-05       SOD         6    720.0  1643.0   \n",
      "\n",
      "   DailyAverageDewPointTemperature  DailyAverageDryBulbTemperature  \\\n",
      "0                             21.0                            38.0   \n",
      "1                             25.0                            41.0   \n",
      "2                             41.0                            47.0   \n",
      "3                             45.0                            46.0   \n",
      "4                             20.0                            39.0   \n",
      "\n",
      "   DailyAverageRelativeHumidity  DailyAverageSeaLevelPressure  \\\n",
      "0                          52.0                         29.76   \n",
      "1                          52.0                         29.91   \n",
      "2                          82.0                         29.81   \n",
      "3                          90.0                         29.62   \n",
      "4                          48.0                         29.84   \n",
      "\n",
      "   DailyAverageStationPressure  ...  DailyCoolingDegreeDays  \\\n",
      "0                        29.62  ...                     0.0   \n",
      "1                        29.77  ...                     0.0   \n",
      "2                        29.67  ...                     0.0   \n",
      "3                        29.49  ...                     0.0   \n",
      "4                        29.69  ...                     0.0   \n",
      "\n",
      "   DailyDepartureFromNormalAverageTemperature  DailyHeatingDegreeDays  \\\n",
      "0                                         4.6                    27.0   \n",
      "1                                         7.7                    24.0   \n",
      "2                                        13.9                    18.0   \n",
      "3                                        13.0                    19.0   \n",
      "4                                         6.1                    26.0   \n",
      "\n",
      "   DailyMaximumDryBulbTemperature  DailyMinimumDryBulbTemperature  \\\n",
      "0                            41.0                            34.0   \n",
      "1                            49.0                            33.0   \n",
      "2                            49.0                            44.0   \n",
      "3                            51.0                            41.0   \n",
      "4                            42.0                            35.0   \n",
      "\n",
      "   DailyPeakWindSpeed  DailyPrecipitation  DailySnowDepth  DailySnowfall  \\\n",
      "0                29.0              0.0000             0.0            0.0   \n",
      "1                22.0              0.0000             0.0            0.0   \n",
      "2                15.0              0.1500             0.0            0.0   \n",
      "3                24.0              0.2700             0.0            0.0   \n",
      "4                43.0              0.0005             0.0            0.0   \n",
      "\n",
      "   DailySustainedWindSpeed  \n",
      "0                     17.0  \n",
      "1                     13.0  \n",
      "2                     10.0  \n",
      "3                     15.0  \n",
      "4                     25.0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('transport_weather.db')\n",
    "\n",
    "# Query the database\n",
    "query = \"SELECT * FROM weather_daily LIMIT 5\"  # Retrieve first 5 rows\n",
    "result = pd.read_sql(query, conn)\n",
    "\n",
    "print(result)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQL Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema.sql file has been successfully generated.\n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "db_path = 'transport_weather.db' # Replace with your actual database file\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Query the sqlite_master table to get schema definitions\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name, sql FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Open a file to write the schema\n",
    "with open(\"schema.sql\", \"w\") as file:\n",
    "    for table_name, schema in cursor.fetchall():\n",
    "        if schema:  # Exclude views or invalid entries\n",
    "            file.write(schema + \";\\n\\n\")\n",
    "\n",
    "conn.close()\n",
    "print(\"Schema.sql file has been successfully generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What’s the most popular hour to take a taxi?\n",
    "# For 01-2020 through 08-2024, show the popularity of Yellow Taxi rides for each hour of the day. \n",
    "QUERY_1_FILENAME = \"yellow_taxi_by_hour_frequency.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%H', pickup_datetime) AS hour,\n",
    "    COUNT(*) AS ride_cnt\n",
    "FROM \n",
    "    yellow_taxi_trips\n",
    "GROUP BY \n",
    "    hour\n",
    "ORDER BY \n",
    "    ride_cnt DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'transport_weather.db'\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('18', 2626),\n",
       " ('17', 2566),\n",
       " ('15', 2498),\n",
       " ('14', 2432),\n",
       " ('16', 2429),\n",
       " ('19', 2330),\n",
       " ('13', 2257),\n",
       " ('12', 2179),\n",
       " ('20', 1961),\n",
       " ('11', 1952),\n",
       " ('10', 1868),\n",
       " ('21', 1758),\n",
       " ('09', 1716),\n",
       " ('22', 1710),\n",
       " ('08', 1471),\n",
       " ('23', 1350),\n",
       " ('07', 1069),\n",
       " ('00', 876),\n",
       " ('06', 587),\n",
       " ('01', 576),\n",
       " ('02', 351),\n",
       " ('05', 234),\n",
       " ('03', 228),\n",
       " ('04', 160)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with conn:\n",
    "    results = conn.execute(QUERY_1).fetchall()\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18:00 is most popular time slot, which is align with our intuition that during that time people are taking taxi/uber to go home or go out for dinner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What’s the most popular day of the week to take an Uber?\n",
    "# For the same time frame, show the popularity of Uber rides for each day of the week.\n",
    "QUERY_2_FILENAME = \"uber_by_day_of_week_frequency.sql\"\n",
    "\n",
    "QUERY_2 = '''\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(*) AS ride_cnt\n",
    "FROM \n",
    "    uber_trips\n",
    "GROUP BY \n",
    "    day_of_week\n",
    "ORDER BY \n",
    "    ride_cnt DESC;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('6', 6336),\n",
       " ('5', 6011),\n",
       " ('4', 5294),\n",
       " ('0', 5247),\n",
       " ('3', 5008),\n",
       " ('2', 4834),\n",
       " ('1', 4454)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with conn:\n",
    "    results = conn.execute(QUERY_2).fetchall()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weekday 6 and 5 are Sunday and Saturday and they're most popular time slot which is align with our intuition that people hang out during these days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: What’s the 95% percentile of trip distance in January 2024?\n",
    "# What is the 95% percentile of distance traveled for all hired rides trips during January 2024?  \n",
    "\n",
    "# The result should be a float. It’s okay if it’s a single float within a list and/or tuple, or a result within a dataframe.\n",
    "QUERY_3_FILENAME = \"travel_distance_Jan2024_95_percent_percentile.sql\"\n",
    "\n",
    "QUERY_3 ='''\n",
    "WITH total AS \n",
    "(\n",
    "    SELECT \n",
    "        trip_distance\n",
    "    FROM \n",
    "        yellow_taxi_trips\n",
    "    WHERE \n",
    "        strftime('%Y-%m', pickup_datetime) = '2024-01'\n",
    "UNION ALL\n",
    "\n",
    "    SELECT \n",
    "        trip_miles AS trip_distance\n",
    "    FROM \n",
    "        uber_trips\n",
    "    WHERE \n",
    "        strftime('%Y-%m', pickup_datetime) = '2024-01'\n",
    "),\n",
    "ordered_total AS\n",
    "(\n",
    "    SELECT\n",
    "        trip_distance,\n",
    "        ROW_NUMBER()OVER(ORDER BY trip_distance) AS row_num,\n",
    "        COUNT(*) OVER() AS total_cnt\n",
    "    FROM\n",
    "        total\n",
    "),\n",
    "percentile AS\n",
    "(\n",
    "    SELECT\n",
    "        trip_distance\n",
    "    FROM\n",
    "        ordered_total\n",
    "    WHERE\n",
    "        row_num >= FLOOR(total_cnt*0.95) AND row_num <= CEIL(total_cnt*0.95)\n",
    ")\n",
    "    SELECT\n",
    "        trip_distance\n",
    "    FROM\n",
    "        percentile\n",
    "    ORDER BY trip_distance\n",
    "    LIMIT 1\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.68"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with conn:\n",
    "    results = conn.execute(QUERY_3).fetchall()\n",
    "results[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query 4: What was the weather like for the busiest days in 2023?\n",
    "What were the top 10 days with the highest number of all hired rides for 2023, and for each day, what was the average distance, average precipitation amount, and average wind speed.\n",
    "\n",
    "The result should be a list of 10 tuples (or a dataframe of 10 rows). Each tuple/row should have five items/columns: a date, an integer for the number of rides, a float for the average distance traveled, a float for the average precipitation amount, and a float the average wind speed. The list of tuples or dataframe should be sorted by total number of rides, descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = \"busiest_day_weather.sql\"\n",
    "\n",
    "QUERY_4 ='''\n",
    "WITH total AS(\n",
    "    SELECT\n",
    "        trip_miles AS trip_distance,\n",
    "        strftime('%Y-%m-%d',pickup_datetime) AS date\n",
    "    FROM\n",
    "        uber_trips\n",
    "    WHERE\n",
    "        strftime('%Y',pickup_datetime)='2023'\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        trip_distance,\n",
    "        strftime('%Y-%m-%d',pickup_datetime) AS date\n",
    "    FROM\n",
    "        yellow_taxi_trips\n",
    "    WHERE\n",
    "        strftime('%Y',pickup_datetime)='2023'\n",
    "),\n",
    "cnt AS(\n",
    "    SELECT\n",
    "        COUNT(*) as ride_cnt,\n",
    "        AVG(trip_distance) AS DailyAverageTripDistance,\n",
    "        date\n",
    "    FROM\n",
    "        total\n",
    "    GROUP BY date\n",
    "),\n",
    "weather AS(\n",
    "    SELECT\n",
    "        DailyAverageWindSpeed,\n",
    "        DailyPrecipitation/24 AS DailyAveragePrecipitation,\n",
    "        strftime('%Y-%m-%d',date) AS date\n",
    "    FROM\n",
    "        weather_daily\n",
    ")\n",
    "    SELECT\n",
    "        cnt.date,\n",
    "        ride_cnt,\n",
    "        DailyAverageTripDistance,\n",
    "        DailyAveragePrecipitation,\n",
    "        DailyAverageWindSpeed\n",
    "    FROM\n",
    "        cnt\n",
    "    LEFT JOIN weather ON cnt.date=weather.date\n",
    "    ORDER BY ride_cnt DESC\n",
    "    LIMIT 10\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020',)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with conn:\n",
    "    results = conn.execute('''select DISTINCT(strftime('%Y',date))from weather_daily''').fetchall()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
